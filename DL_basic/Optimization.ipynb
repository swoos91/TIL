{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "class SGD:\n",
    "    \n",
    "    def __init__(self, lr = 0.01):\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        \n",
    "        for key in params.keys():\n",
    "            \n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD의 단점: 비등방성(anisotropy) 함수에서 탐색 경로가 비효율적\n",
    "- ex. '홀로그램' 과 같이 특정한 방향으로 보면 평소와 다른 성질이 보이는 경우\n",
    "\n",
    "#### SGD의 대안, Momentum, AdaGrad, Adam\n",
    "#### Ref: https://light-tree.tistory.com/140  // https://tensorflow.blog/2017/03/22/momentum-nesterov-momentum/ // https://jjeongil.tistory.com/999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기하학 관점에서 해석하면, \n",
    "# 이전 업데이트 단계의 W 에 대한 손실함수의 기울기 값( 정확하게는 기울기 값 * 학습률에 momentum( 보통 0.9 ) 을 곱한 값 ) ) )과 현 단계에서의 값을 합하여 좀 더 큰 흐름의 방향으로 바꿔 움직임을 크게 만든다.\n",
    "\n",
    "class Momentum:\n",
    "    \n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        \n",
    "        self.v = None\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        \n",
    "        if self.v == None:\n",
    "            \n",
    "            self.v = {}\n",
    "            \n",
    "            for key, val  in params.items():\n",
    "                self.v[ key ] = np.zeros_like( val )\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[ key ] = self.momentum * self.v[ key ] - self.lr * grads[ key ]\n",
    "            params[ key ] += self.v[key]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률에 따른 학습 시간 혹은 발산 문제 발생\n",
    "# 각각의 매개변수( 가중치값 )에 맞게 적응적으로 학습률을 조정하여 위의 문제를 해결함.\n",
    "# 매개변수의 원소 중 크게 갱신된 원소의 학습률을 낮춤.\n",
    "\n",
    "class AdaGrad:\n",
    "    \n",
    "    def __init__(self, lr = 0.01):\n",
    "        \n",
    "        self.lr = 0.01\n",
    "        self.h = None\n",
    "    \n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        \n",
    "        if self.h == None:\n",
    "            \n",
    "            self.h = {}\n",
    "            \n",
    "            for key, val in params.items():\n",
    "                \n",
    "                self.h[ key ] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            \n",
    "            self.h[ key ] += grads[ key ] * grads[ key ]\n",
    "            params[ key ] -= self.lr * grads[ key ] / ( np.sqrt( self.h[ key ] ) + 1e-7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad로 학습이 무힌하게 이루어진다면, 어느 순간 갱신량이 0이 되는 문제가 발생\n",
    "# RMSProp은 '지수이동평균'을 이용하여 과거 기울기의 반영 규모를 기하급수적으로 감소시킴.\n",
    "\n",
    "class RMSProp:\n",
    "    \n",
    "    def __init__(self, lr = 0.01, decay_rate = 0.9 ):\n",
    "        \n",
    "        self.lr = 0.01\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "    \n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        \n",
    "        if self.h is None:\n",
    "            \n",
    "            self.h = {}\n",
    "            \n",
    "            for key, val in params.items():\n",
    "                \n",
    "                self.h[ key ] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            \n",
    "            self.h[ key ] *= self.decay_rate\n",
    "            self.h[ key ] += (1 - self.decay_rate) * grads[ key ] * grads[ key ]\n",
    "            params[ key ] -= self.lr * grads[ key ] / ( np.sqrt( self.h[ key ] ) + 1e-7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "- ref: https://hiddenbeginner.github.io/deeplearning/2019/09/22/optimization_algorithms_in_deep_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum( 이전 업데이트를 고려 ) 과 AdaGrad( 적응적으로 학습률 조정 )를 융합한 기법\n",
    "\n",
    "class Adam:\n",
    "    \n",
    "    def __init__(self, lr = 0.001, beta1 = 0.9, beta2 = 0.999):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.iter = 0\n",
    "    \n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        \n",
    "        if self.m is None:\n",
    "            \n",
    "            self.m, self.v = {}, {}\n",
    "            \n",
    "            for key, val in params.items():\n",
    "                \n",
    "                self.m[ key ] = np.zeros_like( val )\n",
    "                self.v[ key ] = np.zeros_like( val )\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt( 1.0 - self.beta2 ** self.iter ) / (1.0 - self.beta1 ** self.iter) \n",
    "            \n",
    "        for key in params.keys():\n",
    "            \n",
    "            self.m[ key ] += (1 - self.beta1) * (grads[ key ] - self.m[ key ])\n",
    "            self.v[ key ] += (1 - self.beta2) * (grads[ key ]**2 - self.v[ key ])\n",
    "            \n",
    "            params[ key ] -= lr_t * self.m[ key ] / (np.sqrt( self.v[ key ] ) + 1e-7 )        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
